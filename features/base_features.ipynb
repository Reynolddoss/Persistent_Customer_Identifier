{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d118caa78432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moffers_file\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/reynold/github/Persistent_Customer_Identifier/Dataset/offers.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtransactions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m                            \u001b[0menforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massume_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                            \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    363\u001b[0m     read.__doc__ = READ_DOC_TEMPLATE.format(reader=reader_name,\n\u001b[1;32m    364\u001b[0m                                             file_type=file_type)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m                                   \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                                   \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                                   **(storage_options or {}))\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/core.py\u001b[0m in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 raise ValueError('Cannot read compressed files (%s) in byte chunks,'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/core.py\u001b[0m in \u001b[0;36mlogical_size\u001b[0;34m(self, path, compression)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/local.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m\"\"\"Size in bytes of the file at path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv'"
     ]
    }
   ],
   "source": [
    "# set path of the CSV files \n",
    "offers_file       = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/offers.csv\"\n",
    "transactions_file = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv\"\n",
    "#df_tr = dd.read_csv(\"/home/reynold/github/Persistent_Customer_Identifier/Dataset/transactions.csv\",dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flag the test variable \n",
    "testset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if testset:\n",
    "    history_file = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/testHistory.csv\"\n",
    "else:\n",
    "    history_file = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/trainHistory.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduced file \n",
    "reduced_file = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/reduced.csv\"\n",
    "if testset:\n",
    "    folder = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/test/\"\n",
    "else:\n",
    "    folder = \"/home/reynold/github/Persistent_Customer_Identifier/Dataset/train/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output feature file \n",
    "out_file = os.path.join(folder,\"base_feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = [\"offer_id\", \"never_bought_company\", \"never_bought_category\", \"never_bought_brand\", \\\n",
    "\t\"has_bought_brand_company_category\", \"has_bought_brand_category\", \"has_bought_brand_company\", \\\n",
    "\t\"offer_value\", \"total_spend_all\", \"total_spend_ccb\", \"has_bought_company\", \"has_bought_company_q\", \"has_bought_company_a\", \\\n",
    "\t\"has_bought_company_30\", \"has_bought_company_q_30\", \"has_bought_company_a_30\", \"has_bought_company_60\", \\\n",
    "\t\"has_bought_company_q_60\", \"has_bought_company_a_60\", \"has_bought_company_90\", \"has_bought_company_q_90\", \\\n",
    "\t\"has_bought_company_a_90\", \"has_bought_company_180\", \"has_bought_company_q_180\", \"has_bought_company_a_180\", \\\n",
    "\t\"has_bought_category\", \"has_bought_category_q\", \"has_bought_category_a\", \"has_bought_category_30\", \\\n",
    "\t\"has_bought_category_q_30\", \"has_bought_category_a_30\", \"has_bought_category_60\", \"has_bought_category_q_60\", \\\n",
    "\t\"has_bought_category_a_60\", \"has_bought_category_90\", \"has_bought_category_q_90\", \"has_bought_category_a_90\", \\\n",
    "\t\"has_bought_category_180\", \"has_bought_category_q_180\", \"has_bought_category_a_180\", \"has_bought_brand\", \\\n",
    "\t\"has_bought_brand_q\", \"has_bought_brand_a\", \"has_bought_brand_30\", \"has_bought_brand_q_30\", \"has_bought_brand_a_30\", \\\n",
    "\t\"has_bought_brand_60\", \"has_bought_brand_q_60\", \"has_bought_brand_a_60\", \"has_bought_brand_90\", \"has_bought_brand_q_90\", \\\n",
    "\t\"has_bought_brand_a_90\", \"has_bought_brand_180\", \"has_bought_brand_q_180\", \"has_bought_brand_a_180\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_data(offers_file,transactions_file,reduced_file):\n",
    "    offers_cat = {}\n",
    "    offers_co = {}\n",
    "    for e , line in enumerate(open(offers_file)):\n",
    "        offers_cat[line.split(\",\")[1]]=1\n",
    "        offers_co[line.split(\",\")[3]] =1\n",
    "    #getting data of those matching those present in Offers_cat and co\n",
    "    with open(reduced_file,\"wb\") as outfile:\n",
    "        for e, line in enumerate(open(transactions_file)):\n",
    "            if e==0:\n",
    "                outfile.write(line)\n",
    "            else:\n",
    "                if line.split(\",\")[3] in offers_cat or line.split(\",\")[4] in offers_co:\n",
    "                    outfile.write(line)   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(transactions_file,out_file):\n",
    "    offers={}\n",
    "    offers_category={}\n",
    "    offers_company={}\n",
    "    for e, line in enumerate(open(offers_file)):\n",
    "        row = line.strip().split(\",\")\n",
    "        offers[row[0]] = row\n",
    "        offers_category[row[1]] = 1\n",
    "        offers_company[row[3]] = 1\n",
    "    #dict from history\n",
    "    ids = {}\n",
    "    for e,line in enumerate(open(history_file)):\n",
    "        if e>0:\n",
    "            row = line.strip().split(\",\")\n",
    "            ids[row[0]]=row\n",
    "    \n",
    "    seen_ids = set([])\n",
    "    \n",
    "    outfile = open(out_file,\"wb\")\n",
    "    outfile.write(\"Repeat Trips id\" + string.join(feature_set)+\"market chain\\n\")\n",
    "    \n",
    "    #loop through reduced file \n",
    "    last_id = 0\n",
    "    features = defaultdict(float)\n",
    "    for e,line in enumerate(open(transactions_file)):\n",
    "        if e>0:\n",
    "            row = line.strip().split(\",\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
